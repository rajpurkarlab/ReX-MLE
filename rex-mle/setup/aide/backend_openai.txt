"""Backend for OpenAI API."""

import json
import logging
import os
import time
from datetime import datetime
from pathlib import Path

from .utils import FunctionSpec, OutputType, opt_messages_to_list, backoff_create
from funcy import notnone, once, select_values
import openai

logger = logging.getLogger("aide")
requests_logger = None  # Will be initialized per run

_client: openai.OpenAI = None  # type: ignore

OPENAI_TIMEOUT_EXCEPTIONS = (
    openai.RateLimitError,
    openai.APIConnectionError,
    openai.APITimeoutError,
    openai.InternalServerError,
)

# (docs) https://platform.openai.com/docs/guides/function-calling/supported-models
SUPPORTED_FUNCTION_CALL_MODELS = {
    "gpt-4o",
    "gpt-4o-2024-08-06",
    "gpt-4o-2024-05-13",
    "gpt-4o-mini",
    "gpt-4o-mini-2024-07-18",
    "gpt-4-turbo",
    "gpt-4-turbo-2024-04-09",
    "gpt-4-turbo-preview",
    "gpt-4-0125-preview",
    "gpt-4-1106-preview",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo-1106",
    # Azure AI L4 Platform deployment IDs
    "gpt-4.1",
    "gpt-4.1-mini",
    "gpt-4.1-nano",
    "gpt-4o-1120",
    "gpt-4o-0806",
    "gpt-4o-mini-0718",
    "o4-mini-0416",
    "o3-mini-0131",
    "gpt-5",
}


@once
def _setup_openai_client():
    global _client

    # Check if Azure OpenAI configuration is present
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.getenv("AZURE_OPENAI_API_KEY")
    azure_api_version = os.getenv(
        "AZURE_OPENAI_API_VERSION", "2024-12-01-preview")

    if azure_endpoint and azure_api_key:
        logger.info(f"Using Azure OpenAI with endpoint: {azure_endpoint}")
        _client = openai.AzureOpenAI(
            api_key=azure_api_key,
            api_version=azure_api_version,
            azure_endpoint=azure_endpoint,
            max_retries=0
        )
    else:
        logger.info("Using standard OpenAI API")
        _client = openai.OpenAI(max_retries=0)


def is_function_call_supported(model_name: str) -> bool:
    """Return True if the model supports function calling."""
    return model_name in SUPPORTED_FUNCTION_CALL_MODELS


def _get_requests_logger():
    """Get or create the requests logger."""
    global requests_logger
    if requests_logger is None:
        requests_logger = logging.getLogger("aide.requests")
        requests_logger.setLevel(logging.INFO)
        requests_logger.propagate = False

        # Try to find the workspace directory from environment or current working directory
        workspace_dir = os.getenv("WORKSPACE_DIR")
        if workspace_dir:
            log_dir = Path(workspace_dir).parent / "logs"
        else:
            # Fallback to agent logs directory if available
            agent_dir = os.getenv("AGENT_DIR")
            if agent_dir:
                log_dir = Path(agent_dir) / "logs"
            else:
                # Last resort: current directory
                log_dir = Path.cwd() / "logs"

        log_dir.mkdir(parents=True, exist_ok=True)
        requests_log_file = log_dir / "requests.log"

        handler = logging.FileHandler(requests_log_file, mode='a')
        formatter = logging.Formatter('[%(asctime)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        handler.setFormatter(formatter)
        requests_logger.addHandler(handler)

    return requests_logger


def _truncate_text(text: str, max_chars: int = 500) -> str:
    """Truncate text to first and last max_chars characters."""
    if len(text) <= max_chars * 2:
        return text
    return f"{text[:max_chars]}...[{len(text) - max_chars * 2} chars truncated]...{text[-max_chars:]}"


def _log_request_response(messages, model_name, output, req_time, in_tokens, out_tokens):
    """Log OpenAI request and response to requests.log."""
    req_logger = _get_requests_logger()

    # Format request
    request_parts = []
    for msg in messages:
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        if isinstance(content, str):
            truncated = _truncate_text(content)
            request_parts.append(f"[{role.upper()}] {truncated}")

    request_str = "\n".join(request_parts)

    # Format response
    if isinstance(output, str):
        response_str = _truncate_text(output)
    elif isinstance(output, dict):
        response_str = _truncate_text(json.dumps(output, indent=2))
    else:
        response_str = str(output)

    # Log the full interaction
    req_logger.info(
        f"\n{'='*80}\n"
        f"MODEL: {model_name}\n"
        f"REQUEST ({in_tokens} tokens):\n{request_str}\n"
        f"{'-'*80}\n"
        f"RESPONSE ({out_tokens} tokens, {req_time:.2f}s):\n{response_str}\n"
        f"{'='*80}\n"
    )


def query(
    system_message: str | None,
    user_message: str | None,
    func_spec: FunctionSpec | None = None,
    **model_kwargs,
) -> tuple[OutputType, float, int, int, dict]:
    """
    Query the OpenAI API, optionally with function calling.
    Function calling support is only checked for feedback/review operations.
    """
    _setup_openai_client()
    filtered_kwargs: dict = select_values(notnone, model_kwargs)
    model_name = filtered_kwargs.get("model", "")
    logger.debug(f"OpenAI query called with model='{model_name}'")

    messages = opt_messages_to_list(system_message, user_message)

    if func_spec is not None:
        # Only check function call support for feedback/search operations
        if func_spec.name == "submit_review":
            if not is_function_call_supported(model_name):
                logger.warning(
                    f"Review function calling was requested, but model '{model_name}' "
                    "does not support function calling. Falling back to plain text generation."
                )
                filtered_kwargs.pop("tools", None)
                filtered_kwargs.pop("tool_choice", None)
            else:
                filtered_kwargs["tools"] = [func_spec.as_openai_tool_dict]
                filtered_kwargs["tool_choice"] = func_spec.openai_tool_choice_dict

    t0 = time.time()
    completion = backoff_create(
        _client.chat.completions.create,
        OPENAI_TIMEOUT_EXCEPTIONS,
        messages=messages,
        **filtered_kwargs,
    )
    req_time = time.time() - t0

    choice = completion.choices[0]

    if func_spec is None or "tools" not in filtered_kwargs:
        output = choice.message.content
    else:
        tool_calls = getattr(choice.message, "tool_calls", None)

        if not tool_calls:
            logger.warning(
                f"No function call used despite function spec. Fallback to text. "
                f"Message content: {choice.message.content}"
            )
            output = choice.message.content
        else:
            first_call = tool_calls[0]
            assert first_call.function.name == func_spec.name, (
                f"Function name mismatch: expected {func_spec.name}, "
                f"got {first_call.function.name}"
            )
            try:
                output = json.loads(first_call.function.arguments)
            except json.JSONDecodeError as e:
                logger.error(
                    f"Error decoding function arguments:\n{first_call.function.arguments}"
                )
                raise e

    in_tokens = completion.usage.prompt_tokens
    out_tokens = completion.usage.completion_tokens

    info = {
        "system_fingerprint": completion.system_fingerprint,
        "model": completion.model,
        "created": completion.created,
    }

    # Log request and response
    try:
        _log_request_response(messages, model_name, output, req_time, in_tokens, out_tokens)
    except Exception as e:
        logger.warning(f"Failed to log request/response: {e}")

    return output, req_time, in_tokens, out_tokens, info
