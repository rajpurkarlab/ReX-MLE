You are participating in *Med-MLE-bench*, an offline version of Kaggle or Grand Challenge competitions adapted to test the coding ability of agents like yourself.

You will be given a machine learning task. You must solve the task by training a model and running the model on the test set to produce a submission file. You may not hand-label the data to produce a submission file.

BENCHMARK INSTRUCTIONS
------
First, please read the instructions below which are specific to our benchmark setup. Note, these instructions can be found again in `instructions.txt`.

- **Competition-specific instructions**: The instructions for the competition are available in `input/description.md`. Read this file to understand the problem setup for the competition you are currently solving.
- **Dataset**: The dataset files for the competition are available in `input/`.
- **Submission**: We have simplified all competition types to use a simple CSV submission format. Regardless of what the competition instructions say, you MUST produce a submission file at `submission/submission.csv`. We will only evaluate submissions found at this path; failure to submit a `submission.csv` at this path will result in a failed attempt.
- **Test set**: In general, since we do not have access to the original test labels, we have constructed our own train/test split for each competition. As such, the dataset you see in `input/` may differ slightly from what is described in the competition instructions. Nevertheless the files available to you should otherwise be similar to the original and you should proceed with the competition task on the train/test splits available to you.
- **Submission checker**: We provide a tool for checking the validity of your submission file, in the form of a server endpoint at `http://localhost:5001/validate`. If you post a submission file to the server (e.g. `curl -X POST -F "file=@${SUBMISSION_FILE}" ${SERVER_URL}`), you will receive a response message telling you if your submission is valid or not (note that this does NOT give you a score). For your convenience, we provide a script `validate_submission.sh` which can help you make this submission to the validation server.
- **Cheating**: You are not allowed to view others' solutions to this competition. If we detect that your submission is plagiarized from others, the submission will be disqualified.

Your submission will be graded based on the metric in the competition-specific instructions. Your goal is to achieve the best score possible on the competition task.

Wherever there is a conflict between the points above and the competition-specific instructions in `input/description.md`, the above points should always take priority since these are always true for our offline benchmark setup.

ADDITIONAL NOTES
------

- **Compute**: You have access to 1 NVIDIA A100-SXM4-80GB with the appropriate drivers installed.
- **Total Runtime**: You have a maximum of 24hours 0mins 0secs hours to attempt this task. You may iteratively develop your solution within this time frame, but bear in mind that program runtime also counts towards this limit.
- **Total Steps**: You have a maximum of 500 steps to submit your solution.
- You will be informed of the remaining time and steps at each step.
- **Use as much of the available time as possible to refine and optimize your submission**: Your goal is to achieve the best possible score, not just to submit your first working solution. Consider strategies like validating your answer using the "Validate Submission" tool at the http://localhost:5000/validate endpoint, manually spot-checking predictions, building custom validation sets and grading functions, and comparing different algorithms. You may keep working until the time limit expires -- as long as your best submission is stored at the designated location at the end of your run, it will be picked up for grading.

COMPETITION INSTRUCTIONS
------

# NeurIPS 2022 Cell Segmentation Challenge

**Challenge Website:** https://neurips22-cellseg.grand-challenge.org/neurips22-cellseg/
**Year:** 2022
**Conference:** NeurIPS 2022
**Data License:** CC-BY-NC-ND
**Dataset:** https://zenodo.org/records/10719375

## Overview

Cell segmentation is usually the first step for downstream single-cell analysis in microscopy image-based biology and biomedical research. Deep learning has been widely used for image segmentation, but it is hard to collect a large number of labeled cell images to train models because manually annotating cells is extremely time-consuming and costly.

Furthermore, datasets used are often limited to one modality and lacking in diversity, leading to poor generalization of trained models. This competition aims to benchmark cell segmentation methods that could be applied to various microscopy images across multiple imaging platforms and tissue types.

## Task Definition

We frame the cell segmentation problem as a **weakly supervised learning task** to encourage models that use limited labeled and many unlabeled images for cell segmentation, as unlabeled images are relatively easy to obtain in practice.

This is an **instance segmentation** challenge where each cell has an individual label under the same category (cells).

## Challenge Features

1. **Weakly supervised task setting**: Limited labeled patches + many unlabeled images
2. **Versatile algorithms**: Aims to benchmark cell segmentation algorithms across diverse conditions
3. **Large-scale testing**: Testing images include whole-slide images (~10,000×10,000 pixels)
4. **Dual evaluation**: Focus on both segmentation accuracy and computational efficiency

## Data Diversity

Data diversity plays a pivotal role in constructing generalist microscopy image segmentation models. This challenge incorporates diversity from four dimensions:

### 1. Cell Origins
- **Tissue sections**: Cells from diverse tissues, densely packed and spatially organized
- **Cell cultures**: Sparsely distributed and randomly positioned cells under various conditions

### 2. Staining Methods
- **Brightfield microscopy**: Jenner-Giemsa staining
- **Fluorescent microscopy**: Specific antibodies highlighting different cellular structures or proteins

### 3. Microscope Types
- Brightfield microscopy
- Fluorescent microscopy
- Phase-contrast (PC) microscopy
- Differential interference contrast (DIC) microscopy

Each type introduces substantial differences in image characteristics, textures, and associated artifacts.

### 4. Cell Morphologies
- **Round cells**: Most common morphology
- **Elongated cells**: Certain specialized cell types
- **Irregular shapes**: Various other morphologies depending on cell type

## Dataset Structure

### Training Data
- **Labeled patches**: Limited number of annotated cell patches for supervised training
- **Unlabeled images** (Parts 1 & 2): Large collection of unannotated microscopy images for weakly supervised, semi-supervised, or self-supervised learning approaches

### Validation Data
- **Tuning set**: For hyperparameter optimization and model selection

### Test Data
- **Testing set**: Final evaluation including whole-slide images
- Images range from standard patches to large whole-slide images (~10,000×10,000 pixels)

## File Formats
- **Images**: tiff, tif, png, jpg, bmp
- **Annotations**: TIFF format (standardized to preserve maximum information quality)

## Evaluation Metrics

### Primary Metric: F1 Score
- **IoU threshold**: 0.5 (primary)
- **Additional thresholds reported**: 0.6, 0.7, 0.8, 0.9
- **Matching strategy**: Hungarian algorithm (linear_sum_assignment) for optimal instance matching

### Secondary Metrics
- **Precision**: TP / (TP + FP)
- **Recall**: TP / (TP + FN)
- **Dice coefficient**: Binary segmentation quality

### Computational Efficiency
- **GPU memory limit**: 10GB
- **RAM limit**: 28GB
- **Time tolerance**:
  - 10 seconds for images ≤ 1,000,000 pixels (H×W)
  - (H×W) / 1,000,000 × 10 seconds for larger images

### Ranking Scheme
Uses a "rank-then-aggregate" approach:
1. Compute F1 score and running time for each testing case and each team
2. Rank teams for each of the N testing cases (each team gets N×2 rankings)
3. Compute final ranking scores by averaging all rankings and normalizing by number of teams

### Special Handling
- **Boundary cells**: Excluded from evaluation by default (cells touching image borders within 2-pixel margin)
- **Large images**: Evaluated using patch-based approach (2000×2000 ROIs) to handle memory constraints

## Submission Format

Submissions should include:
- Instance segmentation masks where:
  - 0 = background
  - 1, 2, 3, ... = individual cell instances
- File format: TIFF recommended
- One segmentation mask per input image

## Evaluation Code References
- **F1 metric computation**: https://github.com/JunMa11/NeurIPS-CellSeg/blob/main/baseline/compute_metric.py
- **Runtime evaluation**: https://github.com/JunMa11/NeurIPS-CellSeg/blob/main/baseline/cellseg_time_eval.py

## Citation

If you use this dataset, please cite:
```
@article{neurips2022cellseg,
  title={NeurIPS 2022 Cell Segmentation Challenge},
  author={Ma, Jun and others},
  year={2022},
  note={Zenodo record: 10719375}
}
```

## Contact

For questions or to contribute challenging datasets, contact: neurips.cellseg@gmail.com
