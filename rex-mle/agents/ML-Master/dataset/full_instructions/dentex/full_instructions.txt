You are participating in *Med-MLE-bench*, an offline version of Kaggle or Grand Challenge competitions adapted to test the coding ability of agents like yourself.

You will be given a machine learning task. You must solve the task by training a model and running the model on the test set to produce a submission file. You may not hand-label the data to produce a submission file.

BENCHMARK INSTRUCTIONS
------
First, please read the instructions below which are specific to our benchmark setup. Note, these instructions can be found again in `instructions.txt`.

- **Competition-specific instructions**: The instructions for the competition are available in `input/description.md`. Read this file to understand the problem setup for the competition you are currently solving.
- **Dataset**: The dataset files for the competition are available in `input/`.
- **Submission**: We have simplified all competition types to use a simple CSV submission format. Regardless of what the competition instructions say, you MUST produce a submission file at `submission/submission.csv`. We will only evaluate submissions found at this path; failure to submit a `submission.csv` at this path will result in a failed attempt.
- **Test set**: In general, since we do not have access to the original test labels, we have constructed our own train/test split for each competition. As such, the dataset you see in `input/` may differ slightly from what is described in the competition instructions. Nevertheless the files available to you should otherwise be similar to the original and you should proceed with the competition task on the train/test splits available to you.
- **Submission checker**: We provide a tool for checking the validity of your submission file, in the form of a server endpoint at `http://localhost:5001/validate`. If you post a submission file to the server (e.g. `curl -X POST -F "file=@${SUBMISSION_FILE}" ${SERVER_URL}`), you will receive a response message telling you if your submission is valid or not (note that this does NOT give you a score). For your convenience, we provide a script `validate_submission.sh` which can help you make this submission to the validation server.
- **Cheating**: You are not allowed to view others' solutions to this competition. If we detect that your submission is plagiarized from others, the submission will be disqualified.

Your submission will be graded based on the metric in the competition-specific instructions. Your goal is to achieve the best score possible on the competition task.

Wherever there is a conflict between the points above and the competition-specific instructions in `input/description.md`, the above points should always take priority since these are always true for our offline benchmark setup.

ADDITIONAL NOTES
------

- **Compute**: You have access to 1 NVIDIA A100-SXM4-80GB with the appropriate drivers installed.
- **Total Runtime**: You have a maximum of 24hours 0mins 0secs hours to attempt this task. You may iteratively develop your solution within this time frame, but bear in mind that program runtime also counts towards this limit.
- **Total Steps**: You have a maximum of 500 steps to submit your solution.
- You will be informed of the remaining time and steps at each step.
- **Use as much of the available time as possible to refine and optimize your submission**: Your goal is to achieve the best possible score, not just to submit your first working solution. Consider strategies like validating your answer using the "Validate Submission" tool at the http://localhost:5000/validate endpoint, manually spot-checking predictions, building custom validation sets and grading functions, and comparing different algorithms. You may keep working until the time limit expires -- as long as your best submission is stored at the designated location at the end of your run, it will be picked up for grading.

COMPETITION INSTRUCTIONS
------

# DENTEX - Dental Enumeration and Diagnosis on Panoramic X-rays Challenge

## Overview

The DENTEX Challenge (Dental Enumeration and Diagnosis on Panoramic X-rays) was organized in conjunction with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2023. The challenge addresses the critical need for automated analysis of panoramic dental X-rays to assist clinicians in accurate diagnosis and treatment planning.

Panoramic X-rays are widely used in dental practice to provide a comprehensive view of the oral cavity. However, interpreting these images can be time-consuming and prone to errors, especially when general practitioners lack specialized radiology training. This challenge aims to develop AI algorithms that can accurately detect abnormal teeth with dental enumeration and associated diagnosis, helping practitioners carry out procedures with a low margin of error.

## Task Description

The challenge requires participants to develop algorithms that simultaneously perform three hierarchical tasks:

1. **Quadrant Detection (Q)**: Identify which of the four quadrants (1-4) the tooth belongs to, following the FDI numbering system
2. **Tooth Enumeration (N)**: Determine the specific tooth number (1-8) within the identified quadrant
3. **Diagnosis Classification (D)**: Classify the type of dental abnormality present

The output should be bounding boxes around abnormal teeth, each labeled with quadrant (Q), enumeration (N), and diagnosis (D) information.

## Dataset

### Images

- **Modality**: Panoramic X-ray
- **Body Part**: Dental (Teeth and Jaw)
- **Format**: PNG/JPG
- **Patient Age**: 12 years and above
- **Training Set**: 705 panoramic X-ray images with full annotations
- **Test Set**: 50 panoramic X-ray images (validation set, annotations held out)
- **Sources**: Three different medical institutions with varying equipment and imaging protocols

### Annotation System

The dataset uses the **FDI (Fédération Dentaire Internationale) Numbering System**:
- **Quadrants**: 1 (upper right), 2 (upper left), 3 (lower left), 4 (lower right)
- **Tooth Numbers**: 1-8 within each quadrant (1 = front middle tooth, 8 = back molar)
- **Example**: Tooth "48" = Quadrant 4, Tooth 8 (lower right back molar)

### Labels

**Task Type**: Multi-label Object Detection with Hierarchical Classification

**Diagnosis Classes** (4 categories):
1. **Caries**: Tooth decay
2. **Deep Caries**: Advanced tooth decay reaching the pulp
3. **Periapical Lesions**: Infection at the root tip of the tooth
4. **Impacted Teeth**: Teeth that haven't erupted properly through the gum

### Hierarchical Data Structure

The challenge provides three levels of hierarchically annotated data:

1. **Level 1**: 693 X-rays with quadrant detection only
2. **Level 2**: 634 X-rays with quadrant and tooth enumeration
3. **Level 3**: 1,005 X-rays fully annotated (quadrant + enumeration + diagnosis)
   - Training: 705 images
   - Validation: 50 images
   - Testing: 250 images (hidden from participants)

**Additional Resources**:
- 1,571 unlabeled panoramic X-rays available for optional pre-training

### Data Structure

```
train/
├── images/              # 705 panoramic X-ray images
└── annotations.json     # COCO-format annotations with hierarchical labels

test/
└── images/              # 50 test panoramic X-ray images
```

### Annotation Format

Annotations follow COCO JSON format with hierarchical labels:

```json
{
  "images": [...],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "bbox": [x, y, width, height],
      "quadrant_id": 1,      // or "category_id_1"
      "enumeration_id": 3,   // or "category_id_2"
      "diagnosis_id": 1,     // or "category_id_3"
      "area": float,
      "iscrowd": 0
    }
  ],
  "categories_1": [...],  // Quadrant categories
  "categories_2": [...],  // Enumeration categories
  "categories_3": [...]   // Diagnosis categories
}
```

## Evaluation

### Primary Metric

**Mean Average Precision (mAP)** - averaged across all three hierarchical aspects (Quadrant, Enumeration, Diagnosis)

The challenge computes 12 metrics in total (4 metrics × 3 aspects):

### Metrics per Aspect

For each of **Quadrant**, **Enumeration**, and **Diagnosis**:

1. **AP**: Average Precision @ IoU=0.50:0.95 (primary metric for ranking)
2. **AP50**: Average Precision @ IoU=0.50
3. **AP75**: Average Precision @ IoU=0.75
4. **AR**: Average Recall @ IoU=0.50:0.95

**Final Score**: Mean AP = (AP_Quadrant + AP_Enumeration + AP_Diagnosis) / 3

### Evaluation Process

1. Ground truth and predictions are separated by category type (quadrant/enumeration/diagnosis)
2. COCO evaluation metrics are computed independently for each aspect
3. Final ranking is based on the mean AP across all three aspects

### Submission Format

Submissions should include:

1. **submission.csv** with columns:
   ```csv
   image_id,predictions_json
   image_001,predictions/image_001.json
   image_002,predictions/image_002.json
   ...
   ```

2. **predictions/** directory containing COCO-format JSON files:
   ```json
   [
     {
       "image_id": "image_001",
       "bbox": [x, y, width, height],
       "quadrant_id": 1,
       "enumeration_id": 3,
       "diagnosis_id": 1,
       "score": 0.95
     },
     ...
   ]
   ```

**Requirements**:
- `image_id`: Image identifier (filename without extension)
- `bbox`: Bounding box in COCO format [x, y, width, height]
- `quadrant_id`: Integer 1-4
- `enumeration_id`: Integer 1-8
- `diagnosis_id`: Integer 1-4 (1=caries, 2=deep caries, 3=periapical lesion, 4=impacted)
- `score` or `confidence`: Detection confidence (0-1)

## Baseline Method

The challenge baseline is **HierarchicalDet** (Hamamci et al., 2023), a diffusion-based object detection framework designed for multi-label, hierarchically labeled tooth detection.

### Key Components:

1. **Diffusion-based Detection**: Employs a denoising diffusion process to refine initial noisy bounding boxes into precise object boxes
2. **Hierarchical Learning Architecture**: Leverages hierarchically annotated data to utilize previously inferred boxes for enhanced detection
3. **Noisy Box Manipulation**: Novel technique for efficient learning from partial annotations
4. **Multi-label Object Detection**: Custom Detectron2 implementation with selective classification head freezing for unlabeled classes

### Baseline Code

All baseline code is available on GitHub: [DENTEX Repository](https://github.com/ibrahimethemhamamci/DENTEX)

## Challenge Results

The DENTEX Challenge was presented at the DELTA Workshop during MICCAI 2023 (October 8, Vancouver, Canada).

### Top 3 Teams:

1. **sjtu-seiee-426** (SegAndDet) - Mean AP: 0.3995
2. **Chohotech** (tid) - Mean AP: 0.3917
3. **sdent** (first submit) - Mean AP: 0.3685

**Honorable Mentions**: Ai-Align, huikai

## Rules

1. Participants may use any publicly available data for pre-training or augmentation
2. The 1,571 unlabeled X-rays provided may be used for optional pre-training
3. Participants may NOT use additional labeled data from this specific task
4. All methods must be reproducible with provided documentation

## Citation

If you use this dataset, please cite:

```bibtex
@article{hamamci2023dentex,
  title={DENTEX: An Abnormal Tooth Detection with Dental Enumeration and Diagnosis Benchmark for Panoramic X-rays},
  author={Hamamci, Ibrahim Ethem and Er, Sezgin and Simsar, Enis and Yuksel, Atif Emre and Gulhar, Sadullah and Almas, Furkan and Esirgun, Serife Damla and Anagun, Kalin and Gundogar, Gozde and Ekenel, Hazim Kemal and Oksuz, Ilkay and Celik, Emine Elif and Pati, Suraj and Fuernstahl, Philipp and Bakas, Spyridon and Wu, Jiawen and Jin, Dakai and Menze, Bjoern and Chen, Xiaowei and Zhou, Lei and others},
  journal={arXiv preprint arXiv:2305.19112},
  year={2023}
}
```

## Data Source

- **Grand Challenge**: https://dentex.grand-challenge.org/dentex/
- **Zenodo**: https://zenodo.org/records/7812323
- **GitHub**: https://github.com/ibrahimethemhamamci/DENTEX
- **Hugging Face**: https://huggingface.co/datasets/ibrahimhamamci/DENTEX
- **Conference**: MICCAI 2023 - DELTA Workshop

## Additional Resources

- **Challenge Paper**: [arXiv:2305.19112](https://arxiv.org/abs/2305.19112)
- **Workshop Recording**: MICCAI 2023 DELTA Workshop (October 8, 2023)
- **Baseline Method**: HierarchicalDet framework on GitHub
- **Evaluation Code**: COCO-based evaluation using pycocotools

## License

Dataset provided under challenge terms. Please refer to the Grand Challenge page for specific licensing information.
