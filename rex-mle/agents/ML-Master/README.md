# ML-Master: Towards AI-for-AI via Intergration of Exploration and Reasoning

[![project](https://img.shields.io/badge/project-Page-blue)](https://sjtu-sai-agents.github.io/ML-Master/)
[![arXiv](https://img.shields.io/badge/arXiv-2506.16499-b31b1b.svg)](https://arxiv.org/abs/2506.16499)
[![WeChat](https://img.shields.io/badge/WeChat-Êñ∞Êô∫ÂÖÉ-lightgreen)](https://mp.weixin.qq.com/s/8Dn7Hvpmp59-0xDD28nQkw)

> **Status**: ‚åõ Initial code release is now available!

## üöÄ Overview

**ML-Master** is a novel AI4AI (AI-for-AI) agent that integrates exploration and reasoning into a coherent iterative methodology, facilitated by an adaptive memory mechanism that selectively captures and summarizes relevant insights and outcomes, ensuring each component mutually reinforces the other without compromising either. 

![ML-Master](./assets/ML-Master_figure.png)

## üì∞ What's New
- [2025/08/08] Initial code release is now available on GitHub!
- [2025/06/19] Release the preprint version! See the [ArXiv](https://arxiv.org/abs/2506.16499).
- [2025/06/17] Release the initial version! See the initial manuscript [here](./assets/ML-Master_github.pdf).

## üìä Performance Highlights

ML-Master outperforms prior baselines on the **[MLE-Bench](https://github.com/openai/mle-bench)**:

| Metric                      | Result                |
|----------------------------|-----------------------|
| ü•á Average Medal Rate       | **29.3%**             |
| üß† Medium Task Medal Rate   | **20.2%**, more than doubling the previous SOTA            | 
| üïí Runtime Efficiency        | **12 hours**, 50% budget |

![ML-Master](./assets/ML-Master_score.png)


## üìÜ Coming Soon
- [x] Grading report release
- [x] Paper release of ML-Master
- [x] Initial code release of ML-Master (expected early August)
- [ ] Code refactoring for improved readability and maintainability

## üöÄ Quick Start

### üõ†Ô∏è Environment Setup

To get started, make sure to first install the environment of **[MLE-Bench](https://github.com/openai/mle-bench)**. After that, install additional packages based on `requirements.txt`.

```bash
git clone https://github.com/sjtu-sai-agents/ML-Master.git
cd ML-Master
conda create -n ml-master python=3.12
conda activate ml-master

# üîß Install MLE-Bench environment here
# (Follow the instructions in its README)

pip install -r requirements.txt
```

---

### üì¶ Download MLE-Bench Data

The full MLE-Bench dataset is over **2TB**. We recommend downloading and preparing the dataset using the scripts and instructions provided by **[MLE-Bench](https://github.com/openai/mle-bench)**.

Once prepared, the expected dataset structure looks like this:

```
/path/to/mle-bench/plant-pathology-2020-fgvc7/
‚îî‚îÄ‚îÄ prepared
    ‚îú‚îÄ‚îÄ private
    ‚îÇ   ‚îî‚îÄ‚îÄ test.csv
    ‚îî‚îÄ‚îÄ public
        ‚îú‚îÄ‚îÄ description.md
        ‚îú‚îÄ‚îÄ images/
        ‚îú‚îÄ‚îÄ sample_submission.csv
        ‚îú‚îÄ‚îÄ test.csv
        ‚îî‚îÄ‚îÄ train.csv
```

> ü™Ñ ML-Master uses symbolic links to access the dataset. You can download the data to your preferred location and ML-Master will link it accordingly.

---

### üß† Configure DeepSeek and GPT

ML-Master requires LLMs to return custom `<think></think>` tags in the response. Ensure your **DeepSeek** API supports this and follows the `OpenAI` client interface below:

```python
self.client = OpenAI(
    api_key=self.api_key,
    base_url=self.base_url
)
response = self.client.completions.create(**params)
```

Set your `base_url` and `api_key` in the `run.sh` script.
**GPT-4o** is used *only* for evaluation and feedback, consistent with **[MLE-Bench](https://github.com/openai/mle-bench)**.

```bash
# Basic configuration
AGENT_DIR=./
EXP_ID=plant-pathology-2020-fgvc7   # Competition name
dataset_dir=/path/to/mle-bench      # Path to prepared dataset
MEMORY_INDEX=0                      # GPU device ID

# DeepSeek config
code_model=deepseek-r1
code_temp=0.5
code_base_url="your_base_url"
code_api_key="your_api_key"

# GPT config (used for feedback & metrics)
feedback_model=gpt-4o-2024-08-06
feedback_temp=0.5
feedback_base_url="your_base_url"
feedback_api_key="your_api_key"

# CPU allocation
start_cpu=0
CPUS_PER_TASK=18
end_cpu=$((start_cpu + CPUS_PER_TASK - 1))

# Time limit (in seconds)
TIME_LIMIT_SECS=43200
```

### üåê Using Gemini or Claude

Gemini 3 (`gemini-3`, `gemini-3-pro-preview`) and Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`) are supported alongside the OpenAI-compatible models.  
Set `agent.code.model` / `agent.feedback.model` in `config.yaml` (or CLI) to one of these names and provide the corresponding `*_BASE_URL` and `*_API_KEY` environment variables from the `ml-master/gemini3` and `ml-master/claude-sonnet-4.5` presets. Both providers use their native SDKs, so you can point `base_url` at a proxy or leave it empty to hit the default endpoints.

Gemini example (google-genai):
```python
from google import genai

client = genai.Client()
resp = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Find the race condition in this multi-threaded C++ snippet: [code here]",
)
print(resp.text)
```

Claude example (anthropic):
```python
import anthropic

client = anthropic.Anthropic()
resp = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    messages=[{"role": "user", "content": "Hi!"}],
    max_tokens=256,
)
print(resp.content[0].text)
```

---

### ‚ñ∂Ô∏è Start Running
Before running ML-Master, you need to launch a server which tells agent whether the submission is valid or not, allowed and used by MLE-Bench.
```bash
bash launch_server.sh
```

After that, simply run the following command:

```bash
bash run.sh
```

üìù Logs and solutions will be saved in:

* `./logs` (for logs)
* `./workspaces` (for generated solutions)

---
### üìä Evaluation

For evaluation details, please refer to the official **[MLE-Bench evaluation guide](https://github.com/openai/mle-bench)**.


## üôè Acknowledgements

We would like to express our sincere thanks to the following open-source projects that made this work possible:

* üí° **[MLE-Bench](https://github.com/openai/mle-bench)** ‚Äî for providing a comprehensive and professional AutoML benchmarking platform.
* üå≤ **[AIDE](https://github.com/WecoAI/aideml)** ‚Äî for offering a powerful tree-search-based AutoML code framework that inspired parts of our implementation.


## ‚úçÔ∏è Citation

If you find our work helpful, please use the following citations.

```bibtex
@misc{liu2025mlmasteraiforaiintegrationexploration,
      title={ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning}, 
      author={Zexi Liu and Yuzhu Cai and Xinyu Zhu and Yujie Zheng and Runkun Chen and Ying Wen and Yanfeng Wang and Weinan E and Siheng Chen},
      year={2025},
      eprint={2506.16499},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.16499}, 
}
```
