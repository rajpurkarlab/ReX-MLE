# ===============================================================================
# CENTRALIZED ENVIRONMENT CONFIGURATION FOR MED-MLE-BENCH
# ===============================================================================
# This .env file is used by all agents (aide, rdagent, ML-Master)
# Place this file in the rex-mle root directory
# All agents will automatically load these variables

# ===============================================================================
# LLM CREDENTIALS - Shared across agents (AIDE mostly)
# ===============================================================================

# Azure OpenAI Configuration (must use different variables if regular OpenAI endpoint)
AZURE_OPENAI_ENDPOINT="https://azure-ai.hms.edu"
AZURE_OPENAI_API_KEY="SET KEY"
AZURE_OPENAI_API_VERSION="2024-12-01-preview"

# Alternative API Keys (for different providers)
ANTHROPIC_API_KEY="SET KEY"
GEMINI_API_KEY="SET KEY"

# ===============================================================================
# ML-MASTER AGENT CONFIGURATION
# ===============================================================================

# LLM credentials for ML-Master GPT-5 variant
ML_MASTER_GPT5_CODE_BASE_URL=https://azure-ai.hms.edu/openai/deployments/gpt-5/chat/completions?api-version=2024-12-01-preview
ML_MASTER_GPT5_CODE_API_KEY=$AZURE_OPENAI_API_KEY
ML_MASTER_GPT5_FEEDBACK_BASE_URL=https://azure-ai.hms.edu/openai/deployments/gpt-5/chat/completions?api-version=2024-12-01-preview
ML_MASTER_GPT5_FEEDBACK_API_KEY=$AZURE_OPENAI_API_KEY

# Optional convenience duplicates for local scripts (not required by registry)
ML_MASTER_CODE_BASE_URL=$ML_MASTER_GPT5_CODE_BASE_URL
ML_MASTER_CODE_API_KEY=$ML_MASTER_GPT5_CODE_API_KEY
ML_MASTER_FEEDBACK_BASE_URL=$ML_MASTER_GPT5_FEEDBACK_BASE_URL
ML_MASTER_FEEDBACK_API_KEY=$ML_MASTER_GPT5_FEEDBACK_API_KEY

# Optional overrides for ML-Master runtime
ML_MASTER_CODE_MODEL=gpt-5
ML_MASTER_CODE_TEMP=1
ML_MASTER_FEEDBACK_MODEL=gpt-5
ML_MASTER_FEEDBACK_TEMP=1

# ===============================================================================
# RDAGENT CONFIGURATION
# ===============================================================================

# LLM Backend
BACKEND="rdagent.oai.backend.litellm.LiteLLMAPIBackend"

# Model Configuration
CHAT_MODEL="azure/gpt-5"
CHAT_TEMPERATURE=1
EMBEDDING_MODEL="azure/text-embedding-3-small"

# Azure API Settings (for rdagent)
AZURE_API_KEY=$AZURE_OPENAI_API_KEY
AZURE_API_BASE="https://azure-ai.hms.edu"
AZURE_API_VERSION="2024-12-01-preview"

# OpenAI Compatibility (for health checks)
OPENAI_API_KEY=$AZURE_OPENAI_API_KEY
OPENAI_API_BASE="https://azure-ai.hms.edu"

# Reasoning Model Settings (o1-mini specific)
REASONING_THINK_RM=True
MAX_RETRY=10
RETRY_WAIT_SECONDS=20

# LiteLLM Configuration
LITELLM_DROP_PARAMS=True
LITELLM_REQUEST_TIMEOUT=60

# Data paths - Set to rdagent_data directory created by setup_rdagent_data.py
# This prevents RDAgent from trying to use the /kaggle directory
DS_LOCAL_DATA_PATH="./rdagent_data"
KG_LOCAL_DATA_PATH="./rdagent_data"

# Med-MLE-Bench Mode
DS_IF_USING_MLE_DATA=True
KG_IF_USING_MLE_DATA=True

# Competition Settings
# KG_COMPETITION="dentex"  # Uncomment and set specific competition

# Evaluation subdirectory
DS_EVAL_SUB_DIR="eval"

# Timeout settings (in seconds)
DS_DEBUG_TIMEOUT=600                        # 10 minutes for debug phase
DS_FULL_TIMEOUT=3600                        # 1 hour for full runs
DS_CODER_LONGER_TIMEOUT_MULTIPLIER_UPPER=3  # Can extend timeout by 3x

# Loop control
DS_CODER_MAX_LOOP=10     # Maximum code generation iterations
DS_RUNNER_MAX_LOOP=3     # Maximum debugging iterations

# Sampling settings
DS_SAMPLE_DATA_BY_LLM=true  # Let LLM decide how to sample debug data

# Scenario class
DS_SCEN="rdagent.scenarios.data_science.scen.KaggleScen"

# Use 'conda' environment instead of 'docker' to run without Docker
# Options: docker, conda
DS_Coder_CoSTEER_ENV_TYPE="conda"
DS_Runner_CoSTEER_ENV_TYPE="conda"
DS_Runner_CoSTEER_ENV_CONDA_ENV="rexagent"
DS_Coder_CoSTEER_ENV_CONDA_ENV="rexagent"

# Disable Docker requirement check
SKIP_DOCKER_CHECK="true"

DS_ENABLE_MODEL_DUMP=false      # Save trained models
DS_ENABLE_LOG_ARCHIVE=true      # Archive logs after each iteration

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL="DEBUG"

# Caching settings
CACHE_WITH_PICKLE=false         # Use readable text formats instead
DUMP_CHAT_CACHE=false
USE_CHAT_CACHE=false
DUMP_EMBEDDING_CACHE=false
USE_EMBEDDING_CACHE=false

# Logging output
LOG_LLM_CHAT_CONTENT=true       # Enable verbose logging to console

# Hypothesis generation
DS_MAX_TRACE_NUM=1

ENABLE_RESPONSE_SCHEMA=True